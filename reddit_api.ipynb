{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading And Fetching Data From Reddit PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_KEY = 'LyyFU_r17F6s1i0ajYI2dxoSi2dOtw'\n",
    "CLIENT_ID = 'SMi4R1E-3TeMoXqqwPNwUg'\n",
    "USER_NAME = 'Weary-Tooth7440'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\python311\\lib\\site-packages (7.7.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in c:\\python311\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in c:\\python311\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\jamie\\appdata\\roaming\\python\\python311\\site-packages (from praw) (1.5.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\python311\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python311\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', context='talk', palette='Dark2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(client_id=CLIENT_ID,\n",
    "                     client_secret=SECRET_KEY,\n",
    "                     user_agent=USER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display Name: stocks\n",
      "Title: Stocks - Investing and trading for all\n",
      "Description: Almost any post related to stocks is welcome; please read the rules below:\n",
      "\n",
      "[**If you're new here**](https://www.reddit.com/r/stocks/comments/4x1419/if_youre_new_here_read_this_post_first/)\n",
      "\n",
      "##Resources\n",
      "\n",
      "* [Wiki for new investors](https://www.reddit.com/r/stocks/wiki/index)\n",
      "\n",
      "* [Pattern day trading](https://www.reddit.com/r/stocks/wiki/pdtrules)\n",
      "\n",
      "* [Earnings calendar](https://finance.yahoo.com/calendar/earnings/)\n",
      "\n",
      "##Karma requirements\n",
      "\n",
      "[Click here to find how many days old your account needs to be and how much karma you need](https://www.reddit.com/r/stocks/wiki/karma) before you can comment or post to r/Stocks.\n",
      "\n",
      "##Rules [(in depth rules wiki here)](https://www.reddit.com/r/stocks/wiki/rules)\n",
      "\n",
      "1. Disclose any related open positions when discussing a particular stock or financial instrument.\n",
      "\n",
      "2. Spam, ads, solicitations (including referral links), and self-promotion posts or comments will be removed and you might get banned.  Instead, [advertise here.](https://about.reddit.com/advertise/)\n",
      "\n",
      "3. Context & effort must be provided; empty posts or empty posts with links will be automatically removed.  [Low effort mentions for meme stocks will be removed, see here.](https://www.reddit.com/r/stocks/wiki/meme-stocks)\n",
      "\n",
      "4. The Robinhood app should be discussed in /r/Robinhood. Posts regarding this topic will be automatically removed.\n",
      "\n",
      "5. Trolling, insults, or harassment, especially in posts requesting advice, will be removed.\n",
      "\n",
      "6. No bitcoin or crypto discussions unrelated to stocks.  Non-ETF-related Crypto goes on r/CryptoCurrencies [info. ](https://www.reddit.com/r/stocks/wiki/crypto-general)\n",
      "\n",
      "7. No penny stock discussions, including OTC, microcaps, pump & dumps, low vol pumps and SPACs.  Consider posting to r/SPACs, r/pennystocks, or r/weedstocks instead.  [Read here for more info.](https://www.reddit.com/r/stocks/wiki/pennystocks)\n",
      "\n",
      "8. Almost any post related to stocks and investment is welcome on /r/stocks, including pre IPO news, futures & forex related to stocks, and geopolitical or corporate events indicating risks; outside this is offtopic and can be removed.\n",
      "\n",
      "##Filter By Flair\n",
      "\n",
      "* [News](https://www.reddit.com/r/stocks/search?q=flair%3A%22News%22+-flair%3A%22Ticker+News%22&restrict_sr=on&sort=new&t=all)\n",
      "* [Discussion](https://www.reddit.com/r/stocks/search?q=flair%3A%22Discussion%22+-flair%3A%22Ticker+Discussion%22&restrict_sr=on&sort=new&t=all)\n",
      "* [Question](https://www.reddit.com/r/stocks/search?q=flair%3A%22Question%22+-flair%3A%22Ticker+Question%22&restrict_sr=on&sort=new&t=all)\n",
      "* [Advice Request](https://www.reddit.com/r/stocks/search?q=flair%3A%22Advice%2BRequest%22&restrict_sr=on&sort=new&t=all)\n",
      "* [Advice](https://www.reddit.com/r/stocks/search?q=flair%3A%22Advice%22&restrict_sr=on&sort=new&t=all)\n",
      "* [Ticker News](https://www.reddit.com/r/stocks/search?q=flair%3A%22Ticker%2BNews%22&restrict_sr=on&sort=new&t=all)\n",
      "* [Ticker Discussion](https://www.reddit.com/r/stocks/search?q=flair%3A%22Ticker%2BDiscussion%22&restrict_sr=on&sort=new&t=all)\n",
      "* [Ticker Question](https://www.reddit.com/r/stocks/search?q=flair%3A%22Ticker%2BQuestion%22&restrict_sr=on&sort=new&t=all)\n",
      "* [Trades](https://www.reddit.com/r/stocks/search?q=flair%3A%22Trades%22&restrict_sr=on&sort=new&t=all)\n",
      "* [Resources](https://www.reddit.com/r/stocks/search?q=flair%3A%22Resources%22&restrict_sr=on&sort=new&t=all)\n",
      "* [AMA](https://www.reddit.com/r/stocks/search?q=flair%3A%22AMA%22&restrict_sr=on&sort=new&t=all)\n",
      "* [Off-Topic](https://www.reddit.com/r/stocks/search?q=flair%3A%22Off-Topic%22&restrict_sr=on&sort=new&t=all)\n",
      "\n",
      "##Earnings calendar\n",
      "\n",
      "* [Yahoo earnings calendar](https://finance.yahoo.com/calendar/earnings/)\n",
      "* [Bloomberg earnings calendar](https://markets.businessinsider.com/earnings-calendar)\n",
      "\n",
      "##Related Subreddits (see the rules above for related subs as well)\n",
      "\n",
      "* /r/personalfinance\n",
      "* /r/options\n",
      "* /r/thetagang\n",
      "* /r/investing\n",
      "* /r/Economics\n",
      "* /r/StockMarket\n",
      "* /r/Forex\n",
      "* /r/realestate\n",
      "* /r/wallstreetbets\n",
      "\n",
      "###### [Most information to help you learn and practice can be found in our wiki.](https://www.reddit.com/r/stocks/wiki/index)\n",
      "\n",
      "*logo by u/aDrunkLlama.  [Link to logo](https://i.imgur.com/pH7Ceda.png)\n"
     ]
    }
   ],
   "source": [
    "# Subreddit to scrape\n",
    "subreddit = reddit.subreddit('stocks')\n",
    "\n",
    "# Display the name of the Subreddit\n",
    "print(\"Display Name:\", subreddit.display_name)\n",
    " \n",
    "# Display the title of the Subreddit\n",
    "print(\"Title:\", subreddit.title)\n",
    " \n",
    "# Display the description of the Subreddit\n",
    "print(\"Description:\", subreddit.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate My Portfolio - r/Stocks Quarterly Thread June 2024\n",
      "55\n",
      "1d5itn0\n",
      "https://www.reddit.com/r/stocks/comments/1d5itn0/rate_my_portfolio_rstocks_quarterly_thread_june/\n",
      "r/Stocks Daily Discussion Wednesday - Jul 17, 2024\n",
      "9\n",
      "1e5dx4m\n",
      "https://www.reddit.com/r/stocks/comments/1e5dx4m/rstocks_daily_discussion_wednesday_jul_17_2024/\n",
      "Global chip stocks from Nvidia to ASML fall on geopolitics, Trump comments\n",
      "551\n",
      "1e5eubd\n",
      "https://www.reddit.com/r/stocks/comments/1e5eubd/global_chip_stocks_from_nvidia_to_asml_fall_on/\n",
      "These are the stocks on my watchlist (7/17)  \n",
      "16\n",
      "1e5hleu\n",
      "https://www.reddit.com/r/stocks/comments/1e5hleu/these_are_the_stocks_on_my_watchlist_717/\n",
      "ASML Holding GAAP EPS of €4.01, revenue of €6.2B; initiates Q3 and reaffirms FY24 outlook\n",
      "20\n",
      "1e5fovo\n",
      "https://www.reddit.com/r/stocks/comments/1e5fovo/asml_holding_gaap_eps_of_401_revenue_of_62b/\n",
      "Why does semiconductor companies such as TSMC continue to break all-time high?\n",
      "35\n",
      "1e5bdvb\n",
      "https://www.reddit.com/r/stocks/comments/1e5bdvb/why_does_semiconductor_companies_such_as_tsmc/\n",
      "Recent Intel Gaming Chips have >50% Failure Rate\n",
      "392\n",
      "1e4tba1\n",
      "https://www.reddit.com/r/stocks/comments/1e4tba1/recent_intel_gaming_chips_have_50_failure_rate/\n",
      "Deep dive into Ulta Beauty ($ULTA)\n",
      "6\n",
      "1e5k08g\n",
      "https://www.reddit.com/r/stocks/comments/1e5k08g/deep_dive_into_ulta_beauty_ulta/\n",
      "Just bought my first two stocks! Advice\n",
      "4\n",
      "1e5jdai\n",
      "https://www.reddit.com/r/stocks/comments/1e5jdai/just_bought_my_first_two_stocks_advice/\n",
      "Charles Schwab Plummets After Vowing to Shrink Itself Over Time\n",
      "251\n",
      "1e4s3jh\n",
      "https://www.reddit.com/r/stocks/comments/1e4s3jh/charles_schwab_plummets_after_vowing_to_shrink/\n"
     ]
    }
   ],
   "source": [
    "# assume you have a Subreddit instance bound to variable `subreddit`\n",
    "for submission in subreddit.hot(limit=10):\n",
    "    print(submission.title)\n",
    "    # Output: the submission's title\n",
    "    print(submission.score)\n",
    "    # Output: the submission's score\n",
    "    print(submission.id)\n",
    "    # Output: the submission's ID\n",
    "    print(submission.url)\n",
    "    # Output: the URL the submission points to or the submission's URL if it's a self post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit will soon only be available over HTTPS\n",
      "{'_additional_fetch_params': {},\n",
      " '_comments': <praw.models.comment_forest.CommentForest object at 0x00000197C3E6EC50>,\n",
      " '_comments_by_id': {'t1_cs7vwlm': Comment(id='cs7vwlm'),\n",
      "                     't1_cs7xcx2': Comment(id='cs7xcx2'),\n",
      "                     't1_cs7ykx6': Comment(id='cs7ykx6'),\n",
      "                     't1_cs81mem': Comment(id='cs81mem'),\n",
      "                     't1_cs81xp8': Comment(id='cs81xp8'),\n",
      "                     't1_cs82epc': Comment(id='cs82epc'),\n",
      "                     't1_cs82kes': Comment(id='cs82kes'),\n",
      "                     't1_cs83dd8': Comment(id='cs83dd8'),\n",
      "                     't1_cs83ua8': Comment(id='cs83ua8'),\n",
      "                     't1_cs83xhc': Comment(id='cs83xhc'),\n",
      "                     't1_cs846jk': Comment(id='cs846jk'),\n",
      "                     't1_cs847yp': Comment(id='cs847yp'),\n",
      "                     't1_cs848n2': Comment(id='cs848n2'),\n",
      "                     't1_cs84apf': Comment(id='cs84apf'),\n",
      "                     't1_cs84kz5': Comment(id='cs84kz5'),\n",
      "                     't1_cs84n50': Comment(id='cs84n50'),\n",
      "                     't1_cs84poe': Comment(id='cs84poe'),\n",
      "                     't1_cs84rjx': Comment(id='cs84rjx'),\n",
      "                     't1_cs85lql': Comment(id='cs85lql'),\n",
      "                     't1_cs87jic': Comment(id='cs87jic'),\n",
      "                     't1_cs898tl': Comment(id='cs898tl'),\n",
      "                     't1_cs89jvc': Comment(id='cs89jvc'),\n",
      "                     't1_cs8a7pr': Comment(id='cs8a7pr'),\n",
      "                     't1_cs8blse': Comment(id='cs8blse'),\n",
      "                     't1_cs8feqw': Comment(id='cs8feqw'),\n",
      "                     't1_cs8hwfi': Comment(id='cs8hwfi'),\n",
      "                     't1_cs8i7od': Comment(id='cs8i7od'),\n",
      "                     't1_cs8iofx': Comment(id='cs8iofx'),\n",
      "                     't1_cs8j88h': Comment(id='cs8j88h'),\n",
      "                     't1_cs8jh7g': Comment(id='cs8jh7g'),\n",
      "                     't1_cs8jkru': Comment(id='cs8jkru'),\n",
      "                     't1_cs8jns1': Comment(id='cs8jns1'),\n",
      "                     't1_cs8jq4l': Comment(id='cs8jq4l'),\n",
      "                     't1_cs8kbbq': Comment(id='cs8kbbq'),\n",
      "                     't1_cs8kc4z': Comment(id='cs8kc4z'),\n",
      "                     't1_cs8kf07': Comment(id='cs8kf07'),\n",
      "                     't1_cs8km94': Comment(id='cs8km94'),\n",
      "                     't1_cs8knnt': Comment(id='cs8knnt'),\n",
      "                     't1_cs8koyi': Comment(id='cs8koyi'),\n",
      "                     't1_cs8kwd0': Comment(id='cs8kwd0'),\n",
      "                     't1_cs8kxvj': Comment(id='cs8kxvj'),\n",
      "                     't1_cs8l1wk': Comment(id='cs8l1wk'),\n",
      "                     't1_cs8lkbl': Comment(id='cs8lkbl'),\n",
      "                     't1_cs8llxd': Comment(id='cs8llxd'),\n",
      "                     't1_cs8lso1': Comment(id='cs8lso1'),\n",
      "                     't1_cs8mez7': Comment(id='cs8mez7'),\n",
      "                     't1_cs8mhlc': Comment(id='cs8mhlc'),\n",
      "                     't1_cs8p6nn': Comment(id='cs8p6nn'),\n",
      "                     't1_cs8t8u9': Comment(id='cs8t8u9'),\n",
      "                     't1_cs8ts1k': Comment(id='cs8ts1k'),\n",
      "                     't1_cs8x1s6': Comment(id='cs8x1s6'),\n",
      "                     't1_cs905rb': Comment(id='cs905rb'),\n",
      "                     't1_cs974f0': Comment(id='cs974f0'),\n",
      "                     't1_cs9daoa': Comment(id='cs9daoa'),\n",
      "                     't1_cs9dn0k': Comment(id='cs9dn0k'),\n",
      "                     't1_cs9dr4n': Comment(id='cs9dr4n'),\n",
      "                     't1_cs9e0cr': Comment(id='cs9e0cr'),\n",
      "                     't1_cs9ecqc': Comment(id='cs9ecqc'),\n",
      "                     't1_cs9emjk': Comment(id='cs9emjk'),\n",
      "                     't1_cs9gpm0': Comment(id='cs9gpm0'),\n",
      "                     't1_cs9i9q3': Comment(id='cs9i9q3'),\n",
      "                     't1_cs9l636': Comment(id='cs9l636'),\n",
      "                     't1_cs9nc77': Comment(id='cs9nc77'),\n",
      "                     't1_cs9ndy5': Comment(id='cs9ndy5'),\n",
      "                     't1_cs9no26': Comment(id='cs9no26'),\n",
      "                     't1_cs9nweg': Comment(id='cs9nweg'),\n",
      "                     't1_cs9rfqm': Comment(id='cs9rfqm'),\n",
      "                     't1_cs9tp7a': Comment(id='cs9tp7a'),\n",
      "                     't1_cs9tso1': Comment(id='cs9tso1'),\n",
      "                     't1_cs9z8jp': Comment(id='cs9z8jp'),\n",
      "                     't1_csa3tdo': Comment(id='csa3tdo'),\n",
      "                     't1_csa5ckt': Comment(id='csa5ckt'),\n",
      "                     't1_csa7guj': Comment(id='csa7guj'),\n",
      "                     't1_csaf2xj': Comment(id='csaf2xj'),\n",
      "                     't1_csaifdc': Comment(id='csaifdc'),\n",
      "                     't1_csalcjw': Comment(id='csalcjw'),\n",
      "                     't1_csapdsx': Comment(id='csapdsx'),\n",
      "                     't1_csaqqg3': Comment(id='csaqqg3'),\n",
      "                     't1_csav2lc': Comment(id='csav2lc'),\n",
      "                     't1_csay9va': Comment(id='csay9va'),\n",
      "                     't1_csaym10': Comment(id='csaym10'),\n",
      "                     't1_csaz9yt': Comment(id='csaz9yt'),\n",
      "                     't1_csazbjn': Comment(id='csazbjn'),\n",
      "                     't1_csb01td': Comment(id='csb01td'),\n",
      "                     't1_csb0xnn': Comment(id='csb0xnn'),\n",
      "                     't1_csb2iss': Comment(id='csb2iss'),\n",
      "                     't1_csbxpsm': Comment(id='csbxpsm'),\n",
      "                     't1_csc3ini': Comment(id='csc3ini'),\n",
      "                     't1_csghdd8': Comment(id='csghdd8'),\n",
      "                     't1_csh2ggk': Comment(id='csh2ggk'),\n",
      "                     't1_cshasj0': Comment(id='cshasj0'),\n",
      "                     't1_csjbsq9': Comment(id='csjbsq9'),\n",
      "                     't1_cso07fm': Comment(id='cso07fm'),\n",
      "                     't1_csocuyr': Comment(id='csocuyr'),\n",
      "                     't1_csomokx': Comment(id='csomokx'),\n",
      "                     't1_csoncci': Comment(id='csoncci'),\n",
      "                     't1_csoqfop': Comment(id='csoqfop'),\n",
      "                     't1_csoqwaj': Comment(id='csoqwaj'),\n",
      "                     't1_csotkw8': Comment(id='csotkw8'),\n",
      "                     't1_csovx2n': Comment(id='csovx2n'),\n",
      "                     't1_csow6zt': Comment(id='csow6zt'),\n",
      "                     't1_csoycql': Comment(id='csoycql'),\n",
      "                     't1_csp2uvk': Comment(id='csp2uvk'),\n",
      "                     't1_csrxl8t': Comment(id='csrxl8t'),\n",
      "                     't1_csrxya2': Comment(id='csrxya2'),\n",
      "                     't1_cswg4ku': Comment(id='cswg4ku'),\n",
      "                     't1_cswg9cd': Comment(id='cswg9cd'),\n",
      "                     't1_cswk16a': Comment(id='cswk16a'),\n",
      "                     't1_ct3lvx5': Comment(id='ct3lvx5'),\n",
      "                     't1_ct4hrif': Comment(id='ct4hrif'),\n",
      "                     't1_ctd5ycu': Comment(id='ctd5ycu'),\n",
      "                     't1_ctdgeyj': Comment(id='ctdgeyj'),\n",
      "                     't1_cthwcdf': Comment(id='cthwcdf'),\n",
      "                     't1_ctvk404': Comment(id='ctvk404'),\n",
      "                     't1_ctx44kx': Comment(id='ctx44kx'),\n",
      "                     't1_ctx4em5': Comment(id='ctx4em5'),\n",
      "                     't1_ctxh7pl': Comment(id='ctxh7pl'),\n",
      "                     't1_ctyx6vg': Comment(id='ctyx6vg'),\n",
      "                     't1_ctz7auo': Comment(id='ctz7auo'),\n",
      "                     't1_ctzmebe': Comment(id='ctzmebe'),\n",
      "                     't1_cu5ykfz': Comment(id='cu5ykfz'),\n",
      "                     't1_cu8kmrn': Comment(id='cu8kmrn'),\n",
      "                     't1_cuawtww': Comment(id='cuawtww'),\n",
      "                     't1_cucnyw8': Comment(id='cucnyw8'),\n",
      "                     't1_cw9j1ef': Comment(id='cw9j1ef'),\n",
      "                     't1_cwe9ki7': Comment(id='cwe9ki7')},\n",
      " '_fetched': True,\n",
      " '_reddit': <praw.reddit.Reddit object at 0x00000197C08C1410>,\n",
      " 'all_awardings': [],\n",
      " 'allow_live_comments': True,\n",
      " 'approved_at_utc': None,\n",
      " 'approved_by': None,\n",
      " 'archived': True,\n",
      " 'author': Redditor(name='rram'),\n",
      " 'author_flair_background_color': None,\n",
      " 'author_flair_css_class': None,\n",
      " 'author_flair_richtext': [],\n",
      " 'author_flair_template_id': None,\n",
      " 'author_flair_text': None,\n",
      " 'author_flair_text_color': None,\n",
      " 'author_flair_type': 'text',\n",
      " 'author_fullname': 't2_5wfps',\n",
      " 'author_is_blocked': False,\n",
      " 'author_patreon_flair': False,\n",
      " 'author_premium': False,\n",
      " 'awarders': [],\n",
      " 'banned_at_utc': None,\n",
      " 'banned_by': None,\n",
      " 'can_gild': False,\n",
      " 'can_mod_post': False,\n",
      " 'category': None,\n",
      " 'clicked': False,\n",
      " 'comment_limit': 2048,\n",
      " 'comment_sort': 'confidence',\n",
      " 'content_categories': None,\n",
      " 'contest_mode': False,\n",
      " 'created': 1434418540.0,\n",
      " 'created_utc': 1434418540.0,\n",
      " 'discussion_type': None,\n",
      " 'distinguished': 'admin',\n",
      " 'domain': 'self.redditdev',\n",
      " 'downs': 0,\n",
      " 'edited': 1440173665.0,\n",
      " 'gilded': 0,\n",
      " 'gildings': {},\n",
      " 'hidden': False,\n",
      " 'hide_score': False,\n",
      " 'id': '39zje0',\n",
      " 'is_created_from_ads_ui': False,\n",
      " 'is_crosspostable': False,\n",
      " 'is_meta': False,\n",
      " 'is_original_content': False,\n",
      " 'is_reddit_media_domain': False,\n",
      " 'is_robot_indexable': True,\n",
      " 'is_self': True,\n",
      " 'is_video': False,\n",
      " 'likes': None,\n",
      " 'link_flair_background_color': None,\n",
      " 'link_flair_css_class': '',\n",
      " 'link_flair_richtext': [],\n",
      " 'link_flair_text': 'Reddit API',\n",
      " 'link_flair_text_color': None,\n",
      " 'link_flair_type': 'text',\n",
      " 'locked': False,\n",
      " 'media': None,\n",
      " 'media_embed': {},\n",
      " 'media_only': False,\n",
      " 'mod_note': None,\n",
      " 'mod_reason_by': None,\n",
      " 'mod_reason_title': None,\n",
      " 'mod_reports': [],\n",
      " 'name': 't3_39zje0',\n",
      " 'no_follow': False,\n",
      " 'num_comments': 117,\n",
      " 'num_crossposts': 0,\n",
      " 'num_duplicates': 0,\n",
      " 'num_reports': None,\n",
      " 'over_18': False,\n",
      " 'parent_whitelist_status': 'all_ads',\n",
      " 'permalink': '/r/redditdev/comments/39zje0/reddit_will_soon_only_be_available_over_https/',\n",
      " 'pinned': False,\n",
      " 'post_hint': 'self',\n",
      " 'preview': {'enabled': False,\n",
      "             'images': [{'id': 'mKvBKwqPFmnxiYtLQRehhGDWhnrZdJVqzSL_7jJsHb4',\n",
      "                         'resolutions': [{'height': 150,\n",
      "                                          'url': 'https://external-preview.redd.it/L5CgcQzm_oDfAOyXjrsyqxB1cQW9Htc8VyqhoD0wrPU.jpg?width=108&crop=smart&auto=webp&s=4c9874a596b313db7111a5b5e194708dafcf3442',\n",
      "                                          'width': 108}],\n",
      "                         'source': {'height': 200,\n",
      "                                    'url': 'https://external-preview.redd.it/L5CgcQzm_oDfAOyXjrsyqxB1cQW9Htc8VyqhoD0wrPU.jpg?auto=webp&s=600472675b48c5bc261ffab506d0ff52817f3ed6',\n",
      "                                    'width': 144},\n",
      "                         'variants': {}}]},\n",
      " 'pwls': 6,\n",
      " 'quarantine': False,\n",
      " 'removal_reason': None,\n",
      " 'removed_by': None,\n",
      " 'removed_by_category': None,\n",
      " 'report_reasons': None,\n",
      " 'saved': False,\n",
      " 'score': 272,\n",
      " 'secure_media': None,\n",
      " 'secure_media_embed': {},\n",
      " 'selftext': 'Nearly 1 year ago we [gave you the ability to view reddit '\n",
      "             'completely over '\n",
      "             'SSL](http://www.redditblog.com/2014/09/hell-its-about-time-reddit-now-supports.html). '\n",
      "             \"Now we're ready to enforce that everyone use a secure connection \"\n",
      "             'with reddit.\\n'\n",
      "             '\\n'\n",
      "             '**Please ensure that all of your scripts can perform all of '\n",
      "             'their functions over HTTPS by June 29.** At this time we will '\n",
      "             'begin redirecting all site traffic to be over HTTPS and HTTP '\n",
      "             'will no longer be available.\\n'\n",
      "             '\\n'\n",
      "             'If this will be a problem for you, please let us know '\n",
      "             'immediately.\\n'\n",
      "             '\\n'\n",
      "             '**EDIT** 2015-08-21: IT IS DONE. You also have HSTS too.',\n",
      " 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Nearly 1 year ago we <a '\n",
      "                  'href=\"http://www.redditblog.com/2014/09/hell-its-about-time-reddit-now-supports.html\">gave '\n",
      "                  'you the ability to view reddit completely over SSL</a>. Now '\n",
      "                  'we&#39;re ready to enforce that everyone use a secure '\n",
      "                  'connection with reddit.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p><strong>Please ensure that all of your scripts can '\n",
      "                  'perform all of their functions over HTTPS by June '\n",
      "                  '29.</strong> At this time we will begin redirecting all '\n",
      "                  'site traffic to be over HTTPS and HTTP will no longer be '\n",
      "                  'available.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p>If this will be a problem for you, please let us know '\n",
      "                  'immediately.</p>\\n'\n",
      "                  '\\n'\n",
      "                  '<p><strong>EDIT</strong> 2015-08-21: IT IS DONE. You also '\n",
      "                  'have HSTS too.</p>\\n'\n",
      "                  '</div><!-- SC_ON -->',\n",
      " 'send_replies': True,\n",
      " 'spoiler': False,\n",
      " 'stickied': False,\n",
      " 'subreddit': Subreddit(display_name='redditdev'),\n",
      " 'subreddit_id': 't5_2qizd',\n",
      " 'subreddit_name_prefixed': 'r/redditdev',\n",
      " 'subreddit_subscribers': 75194,\n",
      " 'subreddit_type': 'public',\n",
      " 'suggested_sort': None,\n",
      " 'thumbnail': 'self',\n",
      " 'thumbnail_height': None,\n",
      " 'thumbnail_width': None,\n",
      " 'title': 'reddit will soon only be available over HTTPS',\n",
      " 'top_awarded_type': None,\n",
      " 'total_awards_received': 0,\n",
      " 'treatment_tags': [],\n",
      " 'ups': 272,\n",
      " 'upvote_ratio': 0.97,\n",
      " 'url': 'https://www.reddit.com/r/redditdev/comments/39zje0/reddit_will_soon_only_be_available_over_https/',\n",
      " 'user_reports': [],\n",
      " 'view_count': None,\n",
      " 'visited': False,\n",
      " 'whitelist_status': 'all_ads',\n",
      " 'wls': 6}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# assume you have a praw.Reddit instance bound to variable `reddit`\n",
    "submission = reddit.submission(\"39zje0\")\n",
    "print(submission.title)  # to make it non-lazy\n",
    "pprint.pprint(vars(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define lists to store data\n",
    "data = []\n",
    "\n",
    "# Scraping posts & Comments\n",
    "for post in subreddit.top(limit= 10): \n",
    "    #data.append({\n",
    "    #    'Type': 'Post',\n",
    "    #    'Post_id': post.id,\n",
    "    #    'Title': post.title,\n",
    "    #    'Author': post.author.name if post.author else 'Unknown',\n",
    "    #    'Timestamp': post.created_utc,\n",
    "    #    'Text': post.selftext,\n",
    "    #    'Score': post.score,\n",
    "    #    'Total_comments': post.num_comments,\n",
    "    #    'Post_URL': post.url\n",
    "    #})\n",
    "\n",
    "# Check if the post has comments\n",
    "    if post.num_comments > 0:\n",
    "        # Scraping comments for each post\n",
    "        post.comments.replace_more(limit= 5)\n",
    "        for comment in post.comments.list():\n",
    "            data.append({\n",
    "                'id': post.id + '_' +  comment.id ,\n",
    "                'Title': post.title,\n",
    "                'Author': comment.author.name if comment.author else 'Unknown',\n",
    "                'Timestamp': pd.to_datetime(comment.created_utc, unit='s'),\n",
    "                'Text': comment.body,\n",
    "                'Score': comment.score,\n",
    "                'Post_url':post.url,\n",
    "\n",
    "            })\n",
    "            \n",
    "    time.sleep(2)  # Pause for 2 second between requests\n",
    "\n",
    "\n",
    "# Create pandas DataFrame for posts and comments\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropDeletedComment(data):\n",
    "  # Check for rows where the 'Text' column contains '[deleted]'\n",
    "  deleted_rows = data[data['Text'].str.contains('\\[deleted\\]', na=False)]\n",
    "  \n",
    "  # Count the number of deleted rows\n",
    "  num_deleted = len(deleted_rows)\n",
    "  \n",
    "  if (num_deleted) > 1:\n",
    "    #Dropping the text with [deleted]\n",
    "    data = data[~data['Text'].str.contains('\\[deleted\\]', na=False)]\n",
    "    data = data.reset_index(drop=True)\n",
    "  \n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DropDeletedComment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\python311\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python311\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jamie/nltk_data'\n    - 'c:\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jamie\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNLP is amazing! Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms explore its wonders.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jamie/nltk_data'\n    - 'c:\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jamie\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(txt):\n",
    "    # Remove non-word characters and lowercase the text\n",
    "    txt = re.sub(r'\\W+', ' ', txt)\n",
    "    txt = txt.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    word_tokens = word_tokenize(txt)\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_words = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # Stem or Lemmatize each word\n",
    "    stemmed_words = [stemmer.stem(w) for w in filtered_words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in stemmed_words]\n",
    "\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply the preprocessing function to the DataFrame\n",
    "df['original_text'] = df['Text']\n",
    "df['Text'] = df['Text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing most frequent words\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Extracts words into list and count frequency\n",
    "all_words = ' '.join([text for text in df['Text']])\n",
    "all_words = all_words.split()\n",
    "words_df = FreqDist(all_words)\n",
    "\n",
    "# Extracting words and frequency from words_df object\n",
    "words_df = pd.DataFrame({'word':list(words_df.keys()), 'count':list(words_df.values())})\n",
    "\n",
    "# Subsets top 30 words by frequency\n",
    "words_df = words_df.nlargest(columns=\"count\", n = 30)\n",
    "\n",
    "words_df.sort_values('count', inplace = True)\n",
    "\n",
    "# Plotting 30 frequent words\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Top 50 Frequent Word\")\n",
    "ax = plt.barh(words_df['word'], width = words_df['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
