{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading And Fetching Data From Reddit PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_KEY = 'LyyFU_r17F6s1i0ajYI2dxoSi2dOtw'\n",
    "CLIENT_ID = 'SMi4R1E-3TeMoXqqwPNwUg'\n",
    "USER_NAME = 'Weary-Tooth7440'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', context='talk', palette='Dark2')\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "\n",
    "%pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Visualizing most frequent words\n",
    "from nltk.probability import FreqDist\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(client_id=CLIENT_ID,\n",
    "                     client_secret=SECRET_KEY,\n",
    "                     user_agent=USER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_data(subreddit_name):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    return subreddit\n",
    "\n",
    "# Display the name of the Subreddit\n",
    "stocks_subreddit = get_subreddit_data('stocks')\n",
    "wall_street_bets_subreddit = get_subreddit_data('wallstreetbets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_data(subreddit_names):\n",
    "    data = []\n",
    "    for subreddit_name in subreddit_names:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        # ... (rest of your existing code to scrape data)\n",
    "        for post in subreddit.search('daily discussion', sort='new', time_filter='week'):\n",
    "            if post.num_comments > 0:\n",
    "                # Scraping comments for each post\n",
    "                post.comments.replace_more(limit= 5)\n",
    "                for comment in post.comments.list():\n",
    "                    data.append({\n",
    "                        'id': post.id + '_' +  comment.id ,\n",
    "                       'Author': comment.author.name if comment.author else 'Unknown',\n",
    "                        'Timestamp': pd.to_datetime(comment.created_utc, unit='s'),\n",
    "                        'Text': comment.body,\n",
    "                        'Score': comment.score,\n",
    "                        'Post_url':post.url,\n",
    "                    })\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "subreddit_names = ['stocks', 'wallstreetbets']\n",
    "all_data = get_subreddit_data(subreddit_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas DataFrame for posts and comments\n",
    "df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropDeletedComment(data):\n",
    "\n",
    "  #Dropping the text with [deleted] and [removed]\n",
    "  data = data[~data['Text'].str.contains('\\[removed\\]|\\[deleted\\]', na=False, regex = True)]\n",
    "  data = data.reset_index(drop=True)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceParagraphBrake(data):\n",
    "  #Replacing the Paragraph Brake\n",
    "  data['Text'] = data['Text'].str.replace('\\n', ' ')\n",
    "  data = data.reset_index(drop=True)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropSpamComment(data):\n",
    "\n",
    "  #Dropping the text with spam words\n",
    "    data = data[~data['Text'].str.contains('\\b(free|sale|discount|limited time|offer|buy now|click here)\\b', na=False, regex = True)]\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveURL(data):\n",
    "  #Removing the URL\n",
    "  data['Text'] = data['Text'].str.replace(r'http\\S+', '')\n",
    "  data = data.reset_index(drop=True)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveUser(data):\n",
    "  #Removing the User\n",
    "  data['Text'] = data['Text'].str.replace(r'@\\w+', '')\n",
    "  data = data.reset_index(drop=True)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DropDeletedComment(df)\n",
    "df = ReplaceParagraphBrake(df)\n",
    "df = DropSpamComment(df)\n",
    "df = RemoveURL(df)\n",
    "df = RemoveUser(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(txt):\n",
    "    # Remove non-word characters and lowercase the text\n",
    "    txt = re.sub(r'\\W+', ' ', txt)\n",
    "    txt = txt.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    word_tokens = word_tokenize(txt)\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_words = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # Stem or Lemmatize each word\n",
    "    stemmed_words = [stemmer.stem(w) for w in filtered_words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in stemmed_words]\n",
    "\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply the preprocessing function to the DataFrame\n",
    "df['original_text'] = df['Text']\n",
    "df['Text'] = df['Text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extracts words into list and count frequency\n",
    "all_words = ' '.join([text for text in df['Text']])\n",
    "all_words = all_words.split()\n",
    "words_df = FreqDist(all_words)\n",
    "\n",
    "# Extracting words and frequency from words_df object\n",
    "words_df = pd.DataFrame({'word':list(words_df.keys()), 'count':list(words_df.values())})\n",
    "\n",
    "# Subsets top 30 words by frequency\n",
    "words_df = words_df.nlargest(columns=\"count\", n = 30)\n",
    "\n",
    "words_df.sort_values('count', inplace = True)\n",
    "\n",
    "# Plotting 30 frequent words\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Top 50 Frequent Word\")\n",
    "ax = plt.barh(words_df['word'], width = words_df['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    # Tokenize the input text\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)  # Pad and truncate to 3625 tokens)\n",
    "    \n",
    "    # Get model output\n",
    "    output = model(**encoded_input)\n",
    "    \n",
    "    # Calculate softmax scores\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    # Get the predicted label and its score\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    predicted_label = config.id2label[ranking[0]]\n",
    "    predicted_score = scores[ranking[0]]\n",
    "\n",
    "    return predicted_label, predicted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by timestamp in descending order\n",
    "df = df.sort_values(by='Timestamp', ascending=False)\n",
    "\n",
    "# Get the latest 1000 rows\n",
    "latest_df = df.head(10)\n",
    "\n",
    "results = []\n",
    "for index, row in latest_df.iterrows():\n",
    "    text = row['original_text']\n",
    "    timestamp = row['Timestamp']\n",
    "    predicted_label, predicted_score = get_sentiment(text)\n",
    "    results.append([text, timestamp, predicted_label, predicted_score])\n",
    "\n",
    "    # Debugging information (optional)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Timestamp: {timestamp}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "    print(f\"Predicted Score: {predicted_score}\\n\")\n",
    "\n",
    "# Create a Pandas DataFrame from the results\n",
    "results_df = pd.DataFrame(results, columns=['Original Text', 'Timestamp', 'Sentiment', 'Score'])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Frequent Mentioned Stocks\n",
    "\n",
    "# List of stocks to track\n",
    "stocks = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'TSLA', 'META', 'NVDA', 'CRWD']\n",
    "\n",
    "# Initialize a dictionary to store counts\n",
    "stock_counts = {stock: 0 for stock in stocks}\n",
    "\n",
    "# Iterate through the DataFrame and count mentions\n",
    "for _, row in results_df.iterrows():\n",
    "    text = row['Original Text'].lower()\n",
    "    for stock in stocks:\n",
    "        if stock.lower() in text:\n",
    "            stock_counts[stock] += 1\n",
    "\n",
    "# Create a Pandas Series from the counts\n",
    "stock_counts_series = pd.Series(stock_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the Series by count in descending order\n",
    "stock_counts_series = stock_counts_series.sort_values(ascending=False)\n",
    "\n",
    "# Plot the top N most frequently mentioned stocks\n",
    "N = 10  # Change this to the number of stocks you want to display\n",
    "plt.figure(figsize=(10, 6))\n",
    "stock_counts_series.head(N).plot(kind='bar')\n",
    "plt.xlabel('Stock')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top {} Frequently Mentioned Stocks'.format(N))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock_mention = ['CRWD', 'Crowdstrike', 'crowdstrike'] \n",
    "\n",
    "# Create a boolean mask for each word\n",
    "masks = [results_df['Original Text'].str.contains(word, case=False) for word in Stock_mention]\n",
    "\n",
    "# Combine the masks using logical OR\n",
    "combined_mask = np.logical_or.reduce(masks)\n",
    "\n",
    "# Filter the DataFrame using the combined mask\n",
    "filtered_df = results_df[combined_mask]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_score_sum = filtered_df['Score'].where(filtered_df['Sentiment'] == 'positive').sum()\n",
    "negative_score_sum = filtered_df['Score'].where(filtered_df['Sentiment'] == 'negative').sum()\n",
    "neutral_score_sum = filtered_df['Score'].where(filtered_df['Sentiment'] == 'neutral').sum()\n",
    "\n",
    "data = [positive_score_sum, negative_score_sum, neutral_score_sum]\n",
    "Sentiments = ['Positive', 'Negative', 'Neutral']\n",
    " \n",
    "# Creating plot\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.title('Sentiment Analysis on ' + (Stock_mention[0]))\n",
    "plt.pie(data, labels= Sentiments, autopct='%.2f')\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
